{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.6 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Assume that we have some data $x_1, \\ldots, x_n \\in \\mathbb{R}$. Our goal is to find a constant $b$ such that $\\sum_i (x_i - b)^2$ is minimized.\n",
    "\n",
    "1. Find an analytic solution for the optimal value of $b$.\n",
    "2. How does this problem and its solution relate to the normal distribution?\n",
    "3. What if we change the loss from $\\sum_i (x_i - b)^2$ to $\\sum_i |x_i-b|$? Can you find the optimal solution for $b$?\n",
    "\n",
    "å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº›æ•°æ® $x_1, \\ldots, x_n \\in \\mathbb{R}$ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå¸¸æ•° $b$ï¼Œä½¿å¾— $\\sum_i (x_i - b)^2$ æœ€å°åŒ–ã€‚\n",
    "\n",
    "1. æ‰¾åˆ° $b$ çš„æœ€ä¼˜å€¼çš„è§£æè§£ã€‚\n",
    "2. è¿™ä¸ªé—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆä¸æ­£æ€åˆ†å¸ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
    "3. å¦‚æœæˆ‘ä»¬å°†æŸå¤±ä» $\\sum_i (x_i - b)^2$ æ›´æ”¹ä¸º $\\sum_i |x_i-b|$? ä¼šæ€æ ·ï¼Ÿä½ èƒ½æ‰¾åˆ° $b$ çš„æœ€ä¼˜è§£å—ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. $b$ çš„æœ€ä¼˜å€¼çš„è§£æè§£å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\\sum_i (x_i-b)^2 = \\sum_i x_i^2 - 2b\\sum_i x_i + nb^2=n(b-\\frac{1}{n}\\sum_i x_i)^2+\\sum_i x_i^2-\\frac{(\\sum_i x_i)^2}{n}$$\n",
    "è®© $b = \\frac{1}{n}\\sum_i x_i$ï¼Œå‡½æ•°å°†è·å¾—æœ€å°å€¼ã€‚\n",
    "\n",
    "B.\n",
    "\n",
    "å‡è®¾éšæœºå˜é‡ $X = b + \\epsilon$ ï¼Œå…¶ä¸­ ğœ– æ­£æ€åˆ†å¸ƒçš„å™ªå£°, å¦‚æœæˆ‘ä»¬æƒ³ä¼°è®¡å‚æ•° $b$, åˆ™é€šè¿‡ $\\sum_i (x_i - b)^2$ï¼ˆæœ€å°åŒ–æ‰€æœ‰æ ·æœ¬çš„å¹³æ–¹è¯¯å·®å’Œï¼‰ï¼Œ æœ€ç»ˆçš„è§£æ˜¯æ‰€æœ‰æ ·æœ¬çš„å‡å€¼ã€‚\n",
    "\n",
    "C.\n",
    "From the [link](https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-ell-1-norm)\n",
    "\n",
    "![3_1_1](material/3_1_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Prove that the affine functions that can be expressed by $\\mathbf{x}^\\top \\mathbf{w} + b$ are equivalent to linear functions on $(\\mathbf{x}, 1)$.\n",
    "\n",
    "è¯æ˜å¯ä»¥ç”¨ $\\mathbf{x}^\\top \\mathbf{w} + b$ è¡¨ç¤ºçš„ä»¿å°„å‡½æ•°ä¸ $(\\mathbf{x}, 1)$ æ˜¯ç­‰ä»·çš„ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^T\\mathbf{w}+b=x_1*w_1 + ... + x_n * w_n + 1 * b = (\\mathbf{x},1)^T(\\mathbf{w},b)\n",
    "$$\n",
    "\n",
    "$(\\mathbf{x},1)$ ç›¸å½“äºæ‰©å…… 1 åˆ—ï¼ˆå€¼å…¨ä¸º 1ï¼‰ï¼Œè¿™æ · $\\mathbf{x}$ å°±å¯ä»¥ç›´æ¥å’Œ $\\mathbf{w}$ å°† $b$ æ‰©å……åçš„ $\\mathbf{w}$ ç›¸ä¹˜ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Assume that you want to find quadratic functions of $\\mathbf{x}$, i.e., $f(\\mathbf{x}) = b + \\sum_i w_i x_i + \\sum_{j \\leq i} w_{ij} x_{i} x_{j}$. How would you formulate this in a deep network?\n",
    "\n",
    "**å‡è®¾ä½ æƒ³æ‰¾åˆ° $\\mathbf{x}$ çš„äºŒæ¬¡å‡½æ•°ï¼Œæ—¢ $f(\\mathbf{x}) = b + \\sum_i w_i x_i + \\sum_{j \\leq i} w_{ij} x_{i} x_{j}$ã€‚ä½ å°†å¦‚ä½•åœ¨æ·±åº¦ç½‘ç»œä¸­è¡¨è¿°è¿™ä¸€ç‚¹ï¼Ÿ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1457,  0.6503, -0.8702,  1.3687, -0.5948,  0.0442,  0.6812, -2.2838,\n",
      "        -0.5610,  0.0743])\n",
      "tensor([1.2569], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# è®¡ç®—è¾“å…¥å‘é‡ x çš„æ‰€æœ‰å…ƒç´ å¯¹çš„ä¹˜ç§¯ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªä¸€ç»´å¼ é‡ä¸­ã€‚\n",
    "def mul(x):\n",
    "  n = x.size(0)\n",
    "  res = torch.zeros(int(n * (n + 1) / 2))\n",
    "  index = 0\n",
    "  for i in range(0, n):\n",
    "    for j in range(i, n):\n",
    "      res[index] = x[i] * x[j]\n",
    "      index += 1\n",
    "  return res\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, n):\n",
    "    super(Model, self).__init__()\n",
    "    # å®šä¹‰ä¸¤ä¸ªå…¨è¿æ¥å±‚\n",
    "    self.fc1 = nn.Linear(n, 1)\n",
    "    self.fc2 = nn.Linear(int(n * (n + 1) / 2), 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y1 = self.fc1(x)\n",
    "    y2 = mul(x)\n",
    "    y2 = self.fc2(y2)\n",
    "    y = y1 + y2\n",
    "    return y\n",
    "\n",
    "\n",
    "model = Model(10)\n",
    "x = torch.randn(10)\n",
    "y = model(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Recall that one of the conditions for the linear regression problem to be solvable was that the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ has full rank.\n",
    "\n",
    "1. What happens if this is not the case?\n",
    "2. How could you fix it? What happens if you add a small amount of coordinate-wise independent Gaussian noise to all entries of $\\mathbf{X}$?\n",
    "3. What is the expected value of the design matrix $\\mathbf{X}^\\top \\mathbf{X}$ in this case?\n",
    "4. What happens with stochastic gradient descent when $\\mathbf{X}^\\top \\mathbf{X}$ does not have full rank?\n",
    "\n",
    "**è¯·è®°ä½ï¼Œçº¿æ€§å›å½’é—®é¢˜å¯è§£çš„æ¡ä»¶ä¹‹ä¸€æ˜¯è®¾è®¡çŸ©é˜µ $\\mathbf{X}^\\top \\mathbf{X}$ å…·æœ‰æ»¡ç§©ã€‚**\n",
    "\n",
    "1. å¦‚æœæƒ…å†µä¸æ˜¯è¿™æ ·ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n",
    "2. ä½ æ€ä¹ˆèƒ½ä¿®å¤å®ƒï¼Ÿå¦‚æœä½ åœ¨ $\\mathbf{X}$ çš„æ‰€æœ‰æ¡ç›®ä¸­æ·»åŠ å°‘é‡åæ ‡ç‹¬ç«‹çš„é«˜æ–¯å™ªå£°ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n",
    "3. åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®¾è®¡çŸ©é˜µ $\\mathbf{X}^\\top \\mathbf{X}$ çš„æœŸæœ›å€¼æ˜¯å¤šå°‘ï¼Ÿ\n",
    "4. å½“ $\\mathbf{X}^\\top \\mathbf{X}$ ä¸å…·æœ‰æ»¡ç§©æ—¶ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ï¼š\n",
    "\n",
    "1. çº¿æ€§å›å½’é—®é¢˜å¯èƒ½æ— è§£æˆ–è§£ä¸å”¯ä¸€ã€‚\n",
    "2. å¯ä»¥é€šè¿‡æ­£åˆ™åŒ–ï¼ˆå¦‚å²­å›å½’ï¼‰æˆ–æ·»åŠ é«˜æ–¯å™ªå£°æ¥ä½¿ $\\mathbf{X}^\\top \\mathbf{X}$ å…·æœ‰æ»¡ç§©ï¼Œä»è€Œè§£å†³è¯¥é—®é¢˜ã€‚\n",
    "3. æ·»åŠ é«˜æ–¯å™ªå£°åï¼Œ$\\mathbf{X}^\\top \\mathbf{X}$ çš„æœŸæœ›å€¼ä¼šæ¥è¿‘ä¸€ä¸ªæ­£å®šçŸ©é˜µï¼Œä»è€Œç¡®ä¿å…¶å…·æœ‰æ»¡ç§©ã€‚\n",
    "4. éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆSGDï¼‰å¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜è§£æˆ–æ”¶æ•›é€Ÿåº¦å˜æ…¢ã€‚SGD $(\\mathbf{X}^\\top \\mathbf{X})$ ä¸ä¸ºæ»¡ç§©æ—¶ä»ç„¶å¯ä»¥ä½¿ç”¨ï¼Œä½†å¯èƒ½æ— æ³•æ”¶æ•›åˆ°å”¯ä¸€è§£ã€‚è¿™æ˜¯å› ä¸º SGD ä¸ä¾èµ–äº $(\\mathbf{X}^\\top \\mathbf{X})$ çš„å¯é€†æ€§ï¼Œè€Œæ˜¯åŸºäºéšæœºé€‰æ‹©çš„æ•°æ®å­é›†è¿­ä»£æ›´æ–°å›å½’ç³»æ•°ã€‚ç„¶è€Œï¼Œ$(\\mathbf{X})$ ä¸­çº¿æ€§ç›¸å…³åˆ—çš„å­˜åœ¨å¯èƒ½å¯¼è‡´æŸå¤±é¢å…·æœ‰å¤šä¸ªæœ€å°å€¼ï¼Œè¿™æ„å‘³ç€ SGD å¯èƒ½ä¼šæ ¹æ®å›å½’ç³»æ•°çš„åˆå§‹å€¼æ”¶æ•›åˆ°ä¸åŒçš„è§£ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Assume that the noise model governing the additive noise $\\epsilon$ is the exponential distribution. That is, $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$.\n",
    "\n",
    "1. Write out the negative log-likelihood of the data under the model $-\\log P(\\mathbf y \\mid \\mathbf X)$.\n",
    "2. Can you find a closed form solution?\n",
    "3. Suggest a minibatch stochastic gradient descent algorithm to solve this problem. What could possibly go wrong (hint: what happens near the stationary point as we keep on updating the parameters)? Can you fix this?\n",
    "\n",
    "å‡è®¾æ§åˆ¶åŠ æ€§å™ªå£° $\\epsilon$ çš„å™ªå£°æ¨¡å‹æ˜¯æŒ‡æ•°åˆ†å¸ƒã€‚ä¹Ÿå°±æ˜¯è¯´ $p(\\epsilon) = \\frac{1}{2} \\exp(-|\\epsilon|)$ ã€‚\n",
    "\n",
    "1. åœ¨æ¨¡å‹ $-\\log P(\\mathbf y \\mid \\mathbf X)$ ä¸‹å†™å‡ºæ•°æ®çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚\n",
    "2. ä½ èƒ½æ‰¾åˆ°ä¸€ä¸ªå°é—­å½¢å¼çš„è§£å—ï¼Ÿ\n",
    "3. å»ºè®®ä¸€ä¸ªå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¯èƒ½ä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼ˆæç¤ºï¼šå½“æˆ‘ä»¬ä¸æ–­æ›´æ–°å‚æ•°æ—¶ï¼Œé è¿‘é©»ç‚¹ä¼šå‘ç”Ÿä»€ä¹ˆï¼‰ï¼Ÿä½ èƒ½è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ï¼š\n",
    "\n",
    "1. $$P(\\mathbf{y}|\\mathbf{X})=P(\\epsilon=y-Xw)=\\prod \\frac{1}{2}exp(-|y_i-x_i^Tw|)$$\n",
    "   $$L = -logP(\\mathbf{y}|\\mathbf{X})=Nlog2+\\sum_i \\|y_i-x_i^Tw\\|$$\n",
    "\n",
    "2. æ‰¾ä¸åˆ°å°é—­å½¢å¼çš„è§£ã€‚\n",
    "\n",
    "$$\\bigtriangledown_w(L)=-\\sum sign(y_i-xi^Tw)x_i=0$$\n",
    "\n",
    "3. å› æ­¤ï¼Œæ¢¯åº¦ä¸æ˜¯ä¸€ä¸ªå¹³æ»‘çš„å‡½æ•°ï¼Œå¹¶ä¸”åœ¨ $sign(y_i-xi^Tw)$ æ”¹å˜ç¬¦å·çš„åœ°æ–¹æœ‰è·³è·ƒã€‚å› æ­¤ï¼Œè¯¥å‡½æ•°å°†å¾ˆéš¾æ”¶æ•›ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Assume that we want to design a neural network with two layers by composing two linear layers. That is, the output of the first layer becomes the input of the second layer. Why would such a naive composition not work?\n",
    "\n",
    "è®¾æˆ‘ä»¬æƒ³é€šè¿‡ç»„åˆä¸¤ä¸ªçº¿æ€§å±‚æ¥è®¾è®¡ä¸€ä¸ªå…·æœ‰ä¸¤å±‚çš„ç¥ç»ç½‘ç»œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç¬¬ä¸€ä¸ªå±‚çš„è¾“å‡ºæˆä¸ºç¬¬äºŒä¸ªå±‚çš„è¾“å…¥ã€‚ä¸ºä»€ä¹ˆè¿™æ ·çš„ç®€å•ç»„åˆä¸èµ·ä½œç”¨ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ï¼š\n",
    "\n",
    "åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œä¸¤ä¸ªçº¿æ€§å±‚çš„ç»„åˆæœ¬è´¨ä¸Šä¼šå¯¼è‡´ä¸€ä¸ªçº¿æ€§å±‚ã€‚è¿™æ˜¯ç”±äºçº¿æ€§æ€§è´¨ï¼šä¸¤ä¸ªçº¿æ€§å‡½æ•°çš„ç»„åˆæ˜¯å¦ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚\n",
    "\n",
    "è¿™æ ·ï¼Œæ— æ³•å¼•å…¥éçº¿æ€§ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What happens if you want to use regression for realistic price estimation of houses or stock prices?\n",
    "\n",
    "1. Show that the additive Gaussian noise assumption is not appropriate. Hint: can we have negative prices? What about fluctuations?\n",
    "2. Why would regression to the logarithm of the price be much better, i.e., $y = \\log \\textrm{price}$?\n",
    "3. What do you need to worry about when dealing with pennystock, i.e., stock with very low prices? Hint: can you trade at all possible prices? Why is this a bigger problem for cheap stock? For more information review the celebrated Black--Scholes model for option pricing\n",
    "\n",
    "**å¦‚æœæ‚¨æƒ³ä½¿ç”¨å›å½’æ¥å¯¹æˆ¿å±‹æˆ–è‚¡ç¥¨ä»·æ ¼è¿›è¡Œå®é™…ä»·æ ¼ä¼°ç®—ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "1. è¯æ˜åŠ æ€§é«˜æ–¯å™ªå£°å‡è®¾ä¸åˆé€‚ã€‚æç¤ºï¼šæˆ‘ä»¬èƒ½æœ‰è´Ÿä»·æ ¼å—ï¼Ÿæ³¢åŠ¨å‘¢ï¼Ÿ\n",
    "2. ä¸ºä»€ä¹ˆå¯¹ä»·æ ¼çš„å¯¹æ•°è¿›è¡Œå›å½’ä¼šæ›´å¥½ï¼Œå³ $y = \\log \\textrm{price}$ ?\n",
    "3. åœ¨å¤„ç†ä¾¿å£«è‚¡ç¥¨ï¼ˆå³ä»·æ ¼éå¸¸ä½çš„è‚¡ç¥¨ï¼‰æ—¶ï¼Œæ‚¨éœ€è¦æ‹…å¿ƒä»€ä¹ˆï¼Ÿæç¤ºï¼šæ‚¨èƒ½å¦åœ¨æ‰€æœ‰å¯èƒ½çš„ä»·æ ¼è¿›è¡Œäº¤æ˜“ï¼Ÿä¸ºä»€ä¹ˆè¿™å¯¹ä¾¿å®œè‚¡ç¥¨æ¥è¯´æ˜¯ä¸€ä¸ªæ›´å¤§çš„é—®é¢˜ï¼Ÿæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è‘—åçš„å¸ƒè±å…‹-æ–¯ç§‘å°”æ–¯æœŸæƒå®šä»·æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ï¼š\n",
    "\n",
    "1. ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿï¼Œä½†é«˜æ–¯å™ªå£°å…è®¸å‡ºç°è´Ÿå€¼ã€‚è‚¡ç¥¨ä»·æ ¼æ³¢åŠ¨ä¸å…¶å†å²è®°å½•ç›¸å…³ã€‚\n",
    "2. $y$ çš„èŒƒå›´æ˜¯æ•´ä¸ªå®æ•°åŸŸã€‚\n",
    "3. å¦‚æœä»·æ ¼å˜å¾—éå¸¸å¾®å°ï¼Œå…¶å¯¹æ•°å°†å˜ä¸ºä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ•°ï¼Œè¿™ä¼šå¯¼è‡´ä»·å€¼å‰§çƒˆå˜åŒ–ã€‚è¿™äº›è‚¡ç¥¨çš„ä»·æ ¼å¯èƒ½æ— æ³•å¹³ç¨³å˜åŒ–ï¼Œå› ä¸ºæœ€å°å˜åŠ¨å•ä½æˆ–ä»·æ ¼å˜åŒ–çš„æœ€å°å¢é‡ã€‚è¿™å¯èƒ½å¯¼è‡´ä»·æ ¼å˜åŒ–çš„åˆ†å¸ƒä¸è¿ç»­ï¼Œè€Œé«˜æ–¯åˆ†å¸ƒæ— æ³•å¾ˆå¥½åœ°å¯¹å…¶å»ºæ¨¡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Suppose we want to use regression to estimate the _number_ of apples sold in a grocery store.\n",
    "\n",
    "1. What are the problems with a Gaussian additive noise model? Hint: you are selling apples, not oil.\n",
    "2. The [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) captures distributions over counts. It is given by $p(k \\mid \\lambda) = \\lambda^k e^{-\\lambda}/k!$. Here $\\lambda$ is the rate function and $k$ is the number of events you see. Prove that $\\lambda$ is the expected value of counts $k$.\n",
    "3. Design a loss function associated with the Poisson distribution.\n",
    "4. Design a loss function for estimating $\\log \\lambda$ instead.\n",
    "\n",
    "**å‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨å›å½’æ¥ä¼°è®¡æ‚è´§åº—é”€å”®çš„è‹¹æœæ•°é‡ã€‚**\n",
    "\n",
    "1. é«˜æ–¯åŠ æ€§å™ªå£°æ¨¡å‹å­˜åœ¨å“ªäº›é—®é¢˜ï¼Ÿæç¤ºï¼šä½ åœ¨å–è‹¹æœï¼Œè€Œä¸æ˜¯çŸ³æ²¹ã€‚\n",
    "2. [æ³Šæ¾åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Poisson_distribution) æ•æ‰è®¡æ•°çš„åˆ†å¸ƒã€‚å®ƒç”± $p(k \\mid \\lambda) = \\lambda^k e^{-\\lambda}/k!$ ç»™å‡ºã€‚è¿™é‡Œ $\\lambda$ æ˜¯é€Ÿç‡å‡½æ•°ï¼Œ $k$ æ˜¯ä½ çœ‹åˆ°çš„äº‹ä»¶æ•°é‡ã€‚è¯æ˜ $\\lambda$ æ˜¯è®¡æ•° $k$ çš„æœŸæœ›å€¼ã€‚\n",
    "3. è®¾è®¡ä¸æ³Šæ¾åˆ†å¸ƒç›¸å…³çš„æŸå¤±å‡½æ•°ã€‚\n",
    "4. è®¾è®¡ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥ä¼°è®¡ $\\log \\lambda$ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç­”ï¼š\n",
    "\n",
    "1. è‹¹æœçš„æ•°é‡æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå› æ­¤æ˜¯ç¦»æ•£çš„ã€‚ä½†é«˜æ–¯åŠ æ€§å™ªå£°æ˜¯è¿ç»­çš„ã€‚\n",
    "2. $$E(k)=\\sum_{k=1}^\\infty k \\frac{\\lambda^k e^{-\\lambda}}{k!}=\\lambda e^{-\\lambda}\\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k-1)!}=\\lambda e^{-\\lambda}\\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{(k)!}=\\lambda e^{-\\lambda} e^{\\lambda}=\\lambda$$\n",
    "3. $$-log(P(K|\\lambda))=-\\sum_i (k_ilog\\lambda -\\lambda - log(k_i!))= n\\lambda - log\\lambda \\sum_i k_i + \\sum_i (log(k_i!))$$\n",
    "   $\\lambda$ çš„æŸå¤±å‡½æ•°æ˜¯ï¼š\n",
    "   $$L(\\lambda)=n\\lambda - log\\lambda \\sum_i k_i$$\n",
    "4. è®© $t = log\\lambda$\n",
    "   $$L(t)=ne^t - t \\sum_i k_i$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
